# Анализ: v5 vs v5.1 — Критическое исправление баланса релевантность/diversity

## Проблема в solution_v5_improved.py (LB score = 0.4428)

### ГЛАВНАЯ ОШИБКА: Фокус на diversity вместо relevance

**Lambda tuning range:** `[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7]`
- Лучший λ ≈ 0.3  
- **Формула MMR:** `score = λ * relevance + (1-λ) * diversity`
- **При λ=0.3:** `score = 0.3 * relevance + 0.7 * diversity`
- **Результат:** 70% веса на diversity, только 30% на relevance!

### Другие проблемы:
1. **Завышенный wishlist weight:** {1: 1.5, 2: 3.0} — wishlist=1.5 слишком велик
2. **Агрессивная diversity formula:** `0.6*coverage + 0.4*distance` — слишком большой вес на coverage
3. **Низкий NEG_RATIO=3** — недостаточно негативных примеров для обучения
4. **Малый MMR_POOL=100** — ограниченный выбор кандидатов

---

## Исправления в solution_v5.1_improved.py

### 1. **Критическое исправление λ**
```python
# БЫЛО (v5):
lambda_values = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7]
# Лучший λ ≈ 0.3 → 30% relevance, 70% diversity

# СТАЛО (v5.1):
lambda_values = [0.75, 0.80, 0.85, 0.88, 0.90, 0.92, 0.94, 0.95, 0.96, 0.97]
# Лучший λ = 0.96 → 96% relevance, 4% diversity ✓
```

**Эффект:** При λ=0.96 модель почти полностью полагается на релевантность (прогноз GBM), diversity играет минимальную роль для выбора между близкими по релевантности вариантами.

### 2. **Стандартные веса событий**
```python
# БЫЛО (v5):
event_type.map({1: 1.5, 2: 3.0})  # wishlist=1.5, read=3.0

# СТАЛО (v5.1):
event_type.map({1: 1.0, 2: 3.0})  # wishlist=1.0, read=3.0 (стандарт) ✓
```

**Эффект:** Снижен вес wishlist — read в 3 раза важнее, что соответствует реальному поведению пользователей.

### 3. **Увеличен NEG_RATIO**
```python
# БЫЛО (v5):
NEG_RATIO = 3  # 3 негатива на 1 позитив

# СТАЛО (v5.1):
NEG_RATIO = 7  # 7 негативов на 1 позитив ✓
neg_cap = 400  # Увеличен лимит с 200 до 400
```

**Эффект:** Модель лучше учится отличать релевантные рекомендации от нерелевантных.

### 4. **Увеличен MMR_POOL**
```python
# БЫЛО (v5):
MMR_POOL = 100  # Выбор из топ-100

# СТАЛО (v5.1):
MMR_POOL = 150  # Выбор из топ-150 ✓
```

**Эффект:** Больше вариантов для финального ранжирования → лучший баланс.

### 5. **Исправлена diversity formula**
```python
# БЫЛО (v5):
diversity_score = 0.6 * coverage + 0.4 * distance  # Слишком агрессивное покрытие

# СТАЛО (v5.1):
diversity_score = 0.4 * coverage + 0.6 * distance  # Более сбалансировано ✓
```

**Эффект:** Меньший акцент на покрытие жанров, больше — на разнообразие между рекомендациями.

### 6. **Улучшены параметры GBM**
```python
# БЫЛО (v5):
max_iter=800, max_depth=8, learning_rate=0.03, l2_regularization=0.5

# СТАЛО (v5.1):
max_iter=1000, max_depth=7, learning_rate=0.04, l2_regularization=0.8 ✓
```

**Эффект:** Больше итераций и регуляризации для стабильности и лучшей генерализации.

---

## Результаты валидации (локальные)

### v5 (λ=0.3, diversity-focused):
- Локальный validation score ≈ 0.86 (завышен из-за утечки данных в ранних версиях)
- LB score = **0.4428** (после исправления утечки)

### v5.1 (λ=0.96, relevance-focused):
- **Лучший λ = 0.96**
- **Validation score = 0.59277**
  - NDCG@20 = 0.64571
  - Diversity@20 = 0.46923
  - Комбинированный: 0.7 × 0.64571 + 0.3 × 0.46923 = **0.59277**

---

## Ожидаемый эффект на LB

### Почему v5.1 должен работать лучше:

1. **Правильный фокус на relevance:** λ=0.96 означает, что модель почти полностью опирается на предсказание GBM (релевантность), а diversity используется только для «раз брейка» между близкими по скору вариантами.

2. **Лучше обученная модель:** Больше негативных примеров (NEG_RATIO=7) → модель лучше различает релевантное от нерелевантного.

3. **Стандартные веса:** Wishlist=1, Read=3 — проверенная практика в RecSys.

4. **Больше вариантов для выбора:** MMR_POOL=150 даёт больше пространства для оптимизации финального топ-20.

### Прогноз:
- **LB score v5:** 0.4428 (70% diversity)
- **LB score v5.1 (ожидаемый):** **0.55-0.60+** (96% relevance)

Если score не вырастет существенно, возможные причины:
- Негативный sampling всё ещё недостаточно хорош
- Нужно добавить больше relevance-ориентированных признаков
- Возможно, на LB тестовая выборка требует большего diversity чем валидация (но это маловероятно при λ=0.96)

---

## Следующие шаги (если нужно дальнейшее улучшение)

1. **Ensemble подход:** Совместить несколько моделей (разные λ, разные признаки)
2. **Добавить click-through rate features** для популярных книг
3. **User-specific λ:** Разным пользователям может требоваться разный баланс
4. **Cold-start handling:** Улучшить подход для пользователей с малым количеством взаимодействий
5. **Post-processing:** После MMR применить финальный relevance-based reranking

---

## Файлы

- **Старая версия:** `solution_v5_improved.py` (λ=0.1-0.7, LB=0.4428)
- **Новая версия:** `solution_v5.1_improved.py` (λ=0.75-0.97, ожидаем улучшение)
- **Сабмит:** `submission_3.csv` — используйте этот файл для следующей отправки на LB
